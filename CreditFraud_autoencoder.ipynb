{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Autoencoder to Identify Credit Card Fraud\n",
    "*Author: A. Trahan* <br>\n",
    "*Date:   August 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "An autoencoder is a neural netowrk shaped like an hour glass (narrow in the center) that is trained on its own inputs. The goal is to build a simplified latent representation of the inputs (the narrow part) that can still be reliably expanded back to the original dataset. The model is trained on \"good data\", then when anomalous data is submitted to the model it can't be distilled to latent representation the same way, making it easier to separate.\n",
    "\n",
    "This notebook uses classic and neural network methods to attempt to discover credit card fraud. The dataset contains 404 features generated from anaonymized data from a German bank using PCA, so feature engineering options are limited.\n",
    "\n",
    "The process consists of:\n",
    "\n",
    "* Resampled and split the unbalanced dataset\n",
    "* Train an SVM as a base model, aiming for ROC_AUC and Recall\n",
    "* Train an autoencoder\n",
    "    * Visual inspection of autoencoder\n",
    "      * Use t-SNE to plot initial data in $R^2$\n",
    "      * Use t-SNE to plot latent representation in $R^2$\n",
    "    * Train LogReg on latent representation, aiming for ROC_AUC and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O Directories and Files\n",
    "din = './input'\n",
    "dout = './output'\n",
    "fin = 'creditcard.csv'\n",
    "save_figs = True\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(f'{din}/{fin}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling and T/T Split\n",
    "\n",
    "As with many fraud datasets, this one is heavily unbalanced. There are many more non-fraud cases than fraud cases. This can lead a model to be overzealous in marking cases non-fraud, since that correct so often. Along with metric selection, undersampling the majority class can help offset this issue. The resampled dataset is then split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample the majority class\n",
    "nfraud = df.loc[df['Class']==0].sample(1000) # Select only 1000 non-fraud cases (at random)\n",
    "fraud = df.loc[df['Class']==1]               # Select all the fraud cases\n",
    "df_resamp = (nfraud.append(fraud)            # Append the two datasets\n",
    "                   .sample(frac=1)           # Resample to randomize order\n",
    "                   .reset_index(drop=True))  # Reset the index because it's irrellevant\n",
    "\n",
    "# Train/test split\n",
    "X = df_resamp.drop(['Class', 'Time'], axis=1)\n",
    "y = df_resamp['Class']\n",
    "X_train_usamp, X_test_usamp, y_train_usamp, y_test_usamp = train_test_split(X.values, y.values, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SVM Model (SVC)\n",
    "\n",
    "SVM models are a common first step in ML projects (and sometimes the only one needed), so this will be used as a basis against which to compare the quality of predicitons from the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to (0,1)\n",
    "scaler = MinMaxScaler().fit(X_train_usamp)\n",
    "X_train_usamp_sc = scaler.transform(X_train_usamp)\n",
    "X_test_usamp_sc = scaler.transform(X_test_usamp)\n",
    "\n",
    "# Train the model and generate predictions\n",
    "model = SVC(probability=True).fit(X_train_usamp_sc, y_train_usamp)\n",
    "y_tr_pred_usamp = model.predict(X_train_usamp_sc)\n",
    "y_te_pred_usamp = model.predict(X_test_usamp_sc)\n",
    "y_tr_prob_usamp = model.predict_proba(X_train_usamp_sc)\n",
    "y_te_prob_usamp = model.predict_proba(X_test_usamp_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print('Training ROC_AUC: {:.04f}'.format(roc_auc_score(y_train_usamp, y_tr_prob_usamp[:,1])))\n",
    "print('Test ROC_AUC: {:.04f}'.format(roc_auc_score(y_test_usamp, y_te_prob_usamp[:,1])))\n",
    "print('')\n",
    "print('Training Recall: {:.04f}'.format(recall_score(y_train_usamp, y_tr_pred_usamp)))\n",
    "print('Test Recall: {:.04f}'.format(recall_score(y_test_usamp, y_te_pred_usamp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Justification\n",
    "\n",
    "Projecting the training data into two dimensions with t-SNE reveals that it is clearly not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE and plot training data\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_t = tsne.fit_transform(X_train_usamp)\n",
    "\n",
    "fh = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_t[y_train_usamp==0,0], X_t[y_train_usamp==0,1], marker='o', color='g', label='Non Fraud')\n",
    "plt.scatter(X_t[y_train_usamp==1,0], X_t[y_train_usamp==1,1], marker='o', color='r', label='Fraud')\n",
    "plt.title('Fraud/Non-Fraud t-SNE Plot')\n",
    "plt.legend()\n",
    "\n",
    "if save_figs: fh.savefig(f'{dout}/tsne_raw_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Compile the Model\n",
    "\n",
    "The autoencoder is a string of shrinking then growing NN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder\n",
    "X_train = X_train_usamp\n",
    "inp_lyr = Input(shape=(X_train.shape[1],))\n",
    "enc = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(inp_lyr)\n",
    "enc = Dense(50, activation='relu')(enc)\n",
    "dec = Dense(50, activation='relu')(enc)\n",
    "dec = Dense(100, activation='relu')(dec)\n",
    "out_lyr = Dense(X_train.shape[1], activation='relu')(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model and compile\n",
    "autoencoder = Model(inp_lyr, out_lyr)\n",
    "autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "\n",
    "The model is fit to only the non-fraud cases, then any fraud cases will have anomalous latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model training set, min-max scaled non-fraud records\n",
    "X_scale = MinMaxScaler().fit_transform( df.drop(['Class', 'Time'], axis=1).values )\n",
    "x_sc_norm, x_sc_fraud = X_scale[df['Class'].values==0], X_scale[df['Class'].values==1]\n",
    "\n",
    "# Random sample for fitting\n",
    "n_fit_samp = 2000\n",
    "fit_samp = x_sc_norm[np.random.choice(x_sc_norm.shape[0], n_fit_samp, replace=False),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "autoencoder.fit(x=fit_samp, y=fit_samp,\n",
    "               batch_size=256, epochs=15,\n",
    "               shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Latent Represenatations\n",
    "\n",
    "The trained model can be used to create latent representations (parameters from the narrow point in the netowrk). Optimally these are the smallest number of parameters that can still completely define the system, but that's a question for a mathematician. Here we settle for \"small enough, while providing sufficient definition of the system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latent representations\n",
    "hidden_rep = Sequential(autoencoder.layers[:3])\n",
    "norm_hid_rep = hidden_rep.predict(x_sc_norm[np.random.choice(x_sc_norm.shape[0], n_fit_samp*2, replace=False),:])\n",
    "fraud_hid_rep = hidden_rep.predict(x_sc_fraud)\n",
    "\n",
    "X_latent = np.append(norm_hid_rep, fraud_hid_rep, axis=0)\n",
    "y_latent = np.append(np.zeros(norm_hid_rep.shape[0]), np.ones(fraud_hid_rep.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison\n",
    "\n",
    "Projecting the latent representations into two dimensions with t-SNE reveals that it is intuitively more linearly separable than the initial dataset (compare to above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent representations (with t-SNE)\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_t = tsne.fit_transform(X_latent)\n",
    "\n",
    "fh = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_t[y_latent==0,0], X_t[y_latent==0,1], marker='o', color='g', alpha=0.7, label='Non Fraud')\n",
    "plt.scatter(X_t[y_latent==1,0], X_t[y_latent==1,1], marker='o', color='r', alpha=0.7, label='Fraud')\n",
    "plt.title('Fraud/Non-Fraud t-SNE Plot')\n",
    "plt.legend()\n",
    "\n",
    "if save_figs: fh.savefig(f'{dout}/tsne_latent_rep.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Logistic Regression to the Latent Representation\n",
    "\n",
    "Now that the data appears to be linearly separable, a logistic regression should provide reasonable prediction quality at low computational cost. We can create a train-test split on the latent representations and check ROC_AUC and Recall to compare with the SVM from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y_latent, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create and train model\n",
    "model = LogisticRegression(penalty='l1').fit(X_train, y_train)\n",
    "y_tr_pred = model.predict(X_train)\n",
    "y_te_pred = model.predict(X_test)\n",
    "y_tr_prob = model.predict_proba(X_train)\n",
    "y_te_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print('Training ROC_AUC: {:.04f}'.format(roc_auc_score(y_train, y_tr_prob[:,1])))\n",
    "print('Test ROC_AUC: {:.04f}'.format(roc_auc_score(y_test, y_te_prob[:,1])))\n",
    "print('')\n",
    "print('Training Recall: {:.04f}'.format(recall_score(y_train, y_tr_pred)))\n",
    "print('Test Recall: {:.04f}'.format(recall_score(y_test, y_te_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results\n",
    "\n",
    "This is the same t-SNE plot as above, but points are color-coded based on how they were predicted (TN, FP, FN, TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correct/incorrect in tSNE space\n",
    "\n",
    "set_name = 'Undersampling LogReg Test Set'\n",
    "X = X_test\n",
    "y_true = y_test\n",
    "y_pred = y_te_pred\n",
    "\n",
    "def gen_pred_res(y_true, y_pred):\n",
    "    return y_true*10 + y_pred\n",
    "\n",
    "pred_res = gen_pred_res(y_true, y_pred)\n",
    "cases = { 0:{'label':'TN','color':'b', 'alpha':0.5}, \n",
    "          1:{'label':'FP','color':'r', 'alpha':0.8},\n",
    "         10:{'label':'FN','color':'k', 'alpha':0.8},\n",
    "         11:{'label':'TP','color':'g', 'alpha':0.5}}\n",
    "\n",
    "fh = plt.figure(figsize=(10,10))\n",
    "\n",
    "for case, params in cases.items():\n",
    "    plt.scatter(X[pred_res==case,0], X[pred_res==case,1], marker='o', **params)\n",
    "\n",
    "plt.title(f'{set_name} Prediction Errors in t-SNE Space')\n",
    "plt.legend()\n",
    "\n",
    "if save_figs: fh.savefig(f'{dout}/tsne_latent_rep_confusion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "*Data and inspiration from Kaggle kernel: https://www.kaggle.com/shivamb/semi-supervised-classification-using-autoencoders*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
